{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da08f7b",
   "metadata": {},
   "source": [
    "**1. Import the required class from PySpark to create a SparkSession.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b35034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264bae6",
   "metadata": {},
   "source": [
    "**2. Write the statement to create a SparkSession with an appropriate application name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791c999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Tutorial 6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755de86",
   "metadata": {},
   "source": [
    "**3. Display the SparkSession object in the Jupyter Notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95876211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WIN-TBM2Q8JSFF6.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Tutorial 6</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2041deed1e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1367b54",
   "metadata": {},
   "source": [
    "**4. Write the command used to read a CSV file into a PySpark DataFrame with the header option enabled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b697989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('test1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcdb31",
   "metadata": {},
   "source": [
    "**5. Display the contents of the DataFrame after loading the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87faa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c4572",
   "metadata": {},
   "source": [
    "**6. Write the statement used to check the schema of the DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e164aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836f462",
   "metadata": {},
   "source": [
    "**7. Import the required PySpark ML class to convert categorical string columns into numeric form.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70390ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ade07b",
   "metadata": {},
   "source": [
    "**8. Write the command used to apply StringIndexer on a categorical column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2435cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = StringIndexer(inputCol=\"Name\",outputCol=\"NameIndex\")\n",
    "df_name_indexed = ir.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74be1dd",
   "metadata": {},
   "source": [
    "**9. Write the statement used to display the transformed DataFrame after applying StringIndexer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08c5bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+---------+\n",
      "|     Name|age|Experience|Salary|NameIndex|\n",
      "+---------+---+----------+------+---------+\n",
      "|    Krish| 31|        10| 30000|      1.0|\n",
      "|Sudhanshu| 30|         8| 25000|      4.0|\n",
      "|    Sunny| 29|         4| 20000|      5.0|\n",
      "|     Paul| 24|         3| 20000|      2.0|\n",
      "|   Harsha| 21|         1| 15000|      0.0|\n",
      "|  Shubham| 23|         2| 18000|      3.0|\n",
      "+---------+---+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_name_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71eb958",
   "metadata": {},
   "source": [
    "**10. Import the required class used to assemble multiple input columns into a single feature vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e00ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9dab3e",
   "metadata": {},
   "source": [
    "**11. Write the command used to create a VectorAssembler with the specified input columns and output column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d8349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+---------+-------------------+\n",
      "|     Name|age|Experience|Salary|NameIndex|              Input|\n",
      "+---------+---+----------+------+---------+-------------------+\n",
      "|    Krish| 31|        10| 30000|      1.0|[31.0,10.0,30000.0]|\n",
      "|Sudhanshu| 30|         8| 25000|      4.0| [30.0,8.0,25000.0]|\n",
      "|    Sunny| 29|         4| 20000|      5.0| [29.0,4.0,20000.0]|\n",
      "|     Paul| 24|         3| 20000|      2.0| [24.0,3.0,20000.0]|\n",
      "|   Harsha| 21|         1| 15000|      0.0| [21.0,1.0,15000.0]|\n",
      "|  Shubham| 23|         2| 18000|      3.0| [23.0,2.0,18000.0]|\n",
      "+---------+---+----------+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"age\",\"Experience\",\"Salary\"],outputCol=\"Input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1f1fb",
   "metadata": {},
   "source": [
    "**12. Write the statement used to transform the DataFrame using VectorAssembler.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "911ee3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+---------+-------------------+\n",
      "|     Name|age|Experience|Salary|NameIndex|              Input|\n",
      "+---------+---+----------+------+---------+-------------------+\n",
      "|    Krish| 31|        10| 30000|      1.0|[31.0,10.0,30000.0]|\n",
      "|Sudhanshu| 30|         8| 25000|      4.0| [30.0,8.0,25000.0]|\n",
      "|    Sunny| 29|         4| 20000|      5.0| [29.0,4.0,20000.0]|\n",
      "|     Paul| 24|         3| 20000|      2.0| [24.0,3.0,20000.0]|\n",
      "|   Harsha| 21|         1| 15000|      0.0| [21.0,1.0,15000.0]|\n",
      "|  Shubham| 23|         2| 18000|      3.0| [23.0,2.0,18000.0]|\n",
      "+---------+---+----------+------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed = va.transform(df_name_indexed)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062ada5",
   "metadata": {},
   "source": [
    "**13. Import the required PySpark ML class to build a regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38951ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20bd74e",
   "metadata": {},
   "source": [
    "**14. Write the statement used to create a Linear Regression model with the specified features and label columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01f3a4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "`np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeaturesCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExperience\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSalary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\pyspark\\ml\\regression.py:332\u001b[0m, in \u001b[0;36mLinearRegression.__init__\u001b[1;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.ml.regression.LinearRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid\n\u001b[0;32m    330\u001b[0m )\n\u001b[0;32m    331\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\pyspark\\__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\pyspark\\ml\\regression.py:363\u001b[0m, in \u001b[0;36mLinearRegression.setParams\u001b[1;34m(self, featuresCol, labelCol, predictionCol, maxIter, regParam, elasticNetParam, tol, fitIntercept, standardization, solver, weightCol, aggregationDepth, loss, epsilon, maxBlockSizeInMB)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03msetParams(self, \\\\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \\\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m          maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03mSets params for linear regression.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    362\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:503\u001b[0m, in \u001b[0;36mParams._set\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtypeConverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvalid param value given for param \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (p\u001b[38;5;241m.\u001b[39mname, e))\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\pyspark\\ml\\param\\__init__.py:233\u001b[0m, in \u001b[0;36mTypeConverters.toString\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m--> 233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(value) \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring_\u001b[49m, np\u001b[38;5;241m.\u001b[39mstr_, np\u001b[38;5;241m.\u001b[39municode_]:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(value)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amith\\Documents\\GitHub\\misc\\.venv\\lib\\site-packages\\numpy\\__init__.py:413\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __expired_attributes__:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` was removed in the NumPy 2.0 release. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__expired_attributes__[attr]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    416\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchararray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    420\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`np.chararray` is deprecated and will be removed from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe main namespace in the future. Use an array with a string \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor bytes dtype instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: `np.string_` was removed in the NumPy 2.0 release. Use `np.bytes_` instead."
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(featuresCol=[\"age\",\"Experience\",\"Salary\"],labelCol=\"Salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ecf16",
   "metadata": {},
   "source": [
    "**15. Write the command used to fit (train) the Linear Regression model on the prepared DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd00aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1071e62",
   "metadata": {},
   "source": [
    "**16. Write the statement used to display the model coefficients and intercept.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666f770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
